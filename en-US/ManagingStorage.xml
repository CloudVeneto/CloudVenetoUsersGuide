<?xml version='1.0' encoding='utf-8' ?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [
<!ENTITY % BOOK_ENTITIES SYSTEM "Users_Guide.ent">
%BOOK_ENTITIES;
]>
<chapter id="ManagingStorage">
<title>Managing Storage</title>

<para>
There are several ways of handling disk storage in the CloudVeneto:
</para>

<para>

<itemizedlist>

<listitem><para>
<literal>Ephemeral storage</literal> exists only for the life of a virtual machine 
instance. It 
will persist across reboots of the guest operating system but when the 
instance is deleted so is the associated storage. The size of the ephemeral 
storage is defined in the virtual machine flavor.
</para></listitem>

<listitem><para>
<literal>Volumes</literal> are persistent virtualized block devices independent of any 
particular instance. Volumes may be attached to a single instance at a time, 
but may be detached or re-attached to a different instance while retaining 
all data, much like a USB drive. The size of the volume can be selected when 
it is created within the quota limits for the particular project.
</para></listitem>

</itemizedlist>

</para>

<section id="EphemeralStorage">
<title>Ephemeral storage</title>

<para>
Ephemeral storage exists only for the life of a virtual machine instance. 
It will persist across reboots of the guest operating system but when the 
instance is deleted so is the associated storage. The size of the ephemeral 
storage is defined in the virtual machine flavor.
</para>

<para>
Among the flavor details (that are listed in the Dashboard when a VM has to 
be launched or can be seen using the <command>openstack flavor list</command> 
command), there is an attribute called 'Ephemeral'. When you use a flavor 
with an ephemeral disk size different from zero, the instance is booted with 
an extra virtual disk whose size is indicated by the ephemeral value. 
This ephemeral disk can be useful where you want to partition the second disk 
or have a specific disk configuration which is not possible within the system 
disk configuration.
</para>

<warning><para>
Please note that backups are not performed on ephemeral storage systems.
</para></warning>
</section>

<section id="Volumes">
<title>Volumes</title>

<para>
Volumes are persistent virtualized block devices independent of any 
particular instance. Volumes may be attached to a single instance at a time 
(i.e. not like a distributed filesystem such as Lustre or Gluster), but they 
may be detached or re-attached to a different instance while retaining all 
data, much like a USB drive.
</para>

<warning><para>
Please note that backups are not performed on volumes.
</para></warning>


<section id="CreateVolumes">
<title>Create a Volume</title>
<para>
The steps to add a Volume are:
</para>

<para>
Using the Dashboard, click on 
<menuchoice><guimenu>Compute</guimenu><guisubmenu>Volumes</guisubmenu></menuchoice> and then 
<menuchoice><guimenu>Create Volume</guimenu></menuchoice>. 
In the "Create Volume” window specify the name of the volume 
(<replaceable>testvol</replaceable> in the example below) and the desired 
size (<replaceable>12 GB</replaceable> in the example).
As <menuchoice><guimenu>Volume Source</guimenu></menuchoice> specify “No source, empty volume”. 

<mediaobject>
<imageobject>
  <imagedata condition="web" fileref="./images/create_volume.png" />
  <imagedata condition="pdf" fileref="./images/create_volume.png" width="450" />
</imageobject>
</mediaobject>

<mediaobject>
<imageobject>
  <imagedata condition="web" fileref="./images/testvolCreation.png" />
  <imagedata condition="pdf" fileref="./images/testvolCreation.png" width="450" />
</imageobject>
</mediaobject>

</para>

<para>    
Multiple volume types exist, and you need to specify the
type to be used for the volume to be created.
</para>

<note><para>
If you are a University of Padova user, please select the <replaceable>equallogic-unipd</replaceable> 
volume type. 
</para>

<para>
If you are a INFN user, please select the <replaceable>ceph</replaceable> volume type
(the default) unless you have been told by the cloud administrator to use another volume type.
</para></note>

<para>                                                                                       
In general different                    
quotas for the different volume types are set.  
Unfortunately the OpenStack dashboard shows only the overall quota. To see the quota         
per each volume type you need to use the OpenStack CLI (as explained in                      
<xref linkend="AccessingtheCloudthroughCLI"/>) and run the                                   
<literal>cinder quota-usage ${OS_PROJECT_ID}</literal> command.                              
</para>

<para>                                                                                       
E.g.:                                                                                        
                                                                                             
<screen> 
$ cinder quota-usage ${OS_PROJECT_ID}
+----------------------------+--------+----------+-------+
| Type                       | In_use | Reserved | Limit |
+----------------------------+--------+----------+-------+
| backup_gigabytes           | 0      | 0        | 1000  |
| backups                    | 0      | 0        | 10    |
| gigabytes                  | 72     | 0        | 400   |
| gigabytes_ceph             | 48     | 0        | 200   |
| gigabytes_equallogic-unipd | 24     | 0        | 200   |
| gigabytes_iscsi-infnpd     | 0      | 0        | 0     |
| per_volume_gigabytes       | 0      | 0        | 5000  |
| snapshots                  | 0      | 0        | 10    |
| snapshots_ceph             | 0      | 0        | -1    |
| snapshots_equallogic-unipd | 0      | 0        | -1    |
| snapshots_iscsi-infnpd     | 0      | 0        | -1    |
| volumes                    | 8      | 0        | 10    |
| volumes_ceph               | 4      | 0        | -1    |
| volumes_equallogic-unipd   | 4      | 0        | -1    |
| volumes_iscsi-infnpd       | 0      | 0        | -1    |
+----------------------------+--------+----------+-------+
$ 
</screen>
</para>


<para>                                                                                       
In this example the project was given 400 GB. For the ceph and equallogic-unipd
volume types the quota is 200 GB, while it is not possible to create a volume
using the iscsi-infnpd volume type.                                          
</para>

<warning><para>                                                                              
If you try to create a volume using a type for which the quota is over,                      
you will see a generic 'Unable to create volume' error message.                              
</para></warning>


</section>

<section id="AttachVolume">
<title>Using (attaching) a Volume</title>
<para>
The new defined volume will appear in the 
<menuchoice><guimenu>Volumes</guimenu></menuchoice> tab. 
</para>

<para>
To attach this volume to an 
existing instance, click on 
<menuchoice><guimenu>Actions</guimenu><guisubmenu>Manage Attachments</guisubmenu></menuchoice> ... 


<mediaobject>
<imageobject>
  <imagedata condition="web" fileref="./images/AttachVolumeToInstance.png" />
  <imagedata condition="pdf" fileref="./images/AttachVolumeToInstance.png" width="450" />
</imageobject>
</mediaobject>

... select the relevant Virtual Machine...

<mediaobject>
<imageobject>
  <imagedata condition="web" fileref="./images/AttachVolumeToInstance1.jpg" />
  <imagedata condition="pdf" fileref="./images/AttachVolumeToInstance1.jpg" width="450" />
</imageobject>
</mediaobject>

...and click on "Attach Volume".
</para>

<para id="CreateFS">
Log in to the instance and check if the disk has been added:
</para>

<screen>
grep vdb /proc/partitions
 253       16   12582912 vdb
</screen>

<para>
If needed, create a file system on it (this will scratch the disk!):
</para>

<screen>
mkfs -t ext4 /dev/vdb
</screen>

<para>
Mount the volume:
</para>

<screen>
mount /dev/vdb /mnt
</screen>
</section>

<section id="DetachVolume">
<title>Detaching a Volume</title>
<para>
To detach a volume from an instance, first of all log into the virtual 
machine that has the volume mounted, and unmount it:
</para>

<screen>
umount /mnt
</screen>

<para>
Then, using the Dashboard, click on 
<menuchoice><guimenu>Volumes</guimenu></menuchoice>, click on 
<menuchoice><guimenu>More</guimenu><guisubmenu>Manage Attachments</guisubmenu></menuchoice> for the relevant 
volume and select <menuchoice><guimenu>Detach Volume</guimenu></menuchoice>. 
The detached volume can then be associated to another VM, as described above 
(you won't have to re-create the file system, otherwise you will lose the 
content of the volume!)
</para>
</section>

<section id="DeleteVolume">
<title>Deleting a Volume</title>
<para>
If a volume is not needed any longer, to completely remove it (note that 
this step cannot be reverted!):
</para>

<itemizedlist>

<listitem><para>
if needed, detach the volume from the associated instance
</para></listitem>


<listitem><para>
using the Dashboard, click on 
<menuchoice><guimenu>Compute</guimenu><guisubmenu>Volumes</guisubmenu></menuchoice>, select the relevant 
volume and then select 
<menuchoice><guimenu>Delete Volumes</guimenu></menuchoice>
</para></listitem>

</itemizedlist>
</section>


<section id="SharingVolume">
<title>Sharing a volume between multiple (virtual) machines </title>

<para>
As discussed in <xref linkend="Volumes"/>, a volume may be attached to a 
single instance. However it can be shared with other virtual machines of the 
Cloud (and/or with other hosts) using NFS.
</para>

<orderedlist>
<listitem>
	<para><literal>Configure NFS server</literal></para>
	<orderedlist>
		<listitem>
			<para>
			Once a volume has been created, formatted and attached to this instance
			acting as NFS server, create the mount point and mount the 
			volume on this virtual machine:
<screen>
mkdir /dataNfs
mount /dev/sdb /dataNfs
</screen>
			</para>
		</listitem>

		<listitem>
			<para>
			Ensure that on this virtual machine the packages providing the <command>rpc.nfsd</command> daemon are installed:
<screen>
# yum whatprovides "*/rpc.nfsd"

Loaded plugins: dellsysid, fastestmirror
Loading mirror speeds from cached hostfile
 * base: centos.fastbull.org
 * epel: mirror.23media.de
 * extras: centos.fastbull.org
 * updates: fr2.rpmfind.net
1:nfs-utils-1.3.0-0.8.el7.x86_64 : NFS utilities and supporting clients and daemons for the kernel NFS server
Repo        : base
Matched from:
Filename    : /usr/sbin/rpc.nfsd



1:nfs-utils-1.3.0-0.8.el7.x86_64 : NFS utilities and supporting clients and daemons for the kernel NFS server
Repo        : @base
Matched from:
Filename    : /usr/sbin/rpc.nfsd

# yum install nfs-utils
</screen>
			</para>
		</listitem>
		<listitem>
			<para>
			Insert the correct export directive in the /etc/exports file. For example 
			if the volume must be visible in read-only mode to
			all the virtual machines of the same subnet 10.67.1.* (check the subnet with 
			the <command>ifconfig</command> command) the content of the /etc/exports file 
			will be:
<screen>
/dataNfs 10.67.1.*(ro,no_root_squash)
</screen>
            Note that there are no spaces between the '*' and the '('.
			</para>
                        <para>
If the volume must be visible in read-write to all the virtual machines of the same subnet 
10.67.1.*, you                                                                               
might export it using the async or sync option.                                              
In short: async is much faster wrt sync, but can lead to data corruption if the server crashes 
during write operations                                                                   
(async means that the NFS server will acknowledge data                                       
before it's committed to disk, while sync does the opposite: the server will only acknowledge 
data after it's written out). Sync can be slow in particular                               
when you have to write many files, since the open()/creat()                                  
and close() system calls have to wait for the new data to hit disk. 
			</para>

<para>                                                                                       
To export a volume using the async option the /etc/exports will be something like:           
<screen>                                                                                     
/dataNfs 10.67.1.*(async,rw,no_root_squash)                                                 
</screen>
</para>

<para>                                                                                       
To export a volume using the sync option the /etc/exports will be something like:            
<screen>                                                                                     
/dataNfs 10.67.1.*(sync,rw,no_root_squash)                                                  
</screen>
</para>

		</listitem>
		<listitem>
			<para>
			Check the firewall on the virtual machine. Ensure that the other instances
			have both UDP and TCP access to ports 111, 2049 and 875:</para>
			<itemizedlist> <!-- iptables/firewalld -->
				<listitem>
				<para>
				For newer distributions using <command>firewalld</command> issue the following
				commands:
<screen>
firewall-cmd --add-service=nfs
firewall-cmd --permanent --add-service=nfs
</screen>
				</para>
				</listitem>
				<listitem>
				<para>
				For older distributions using iptables, the /etc/sysconfig/iptables file should be 
				something like this:
<screen>
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 111 -j ACCEPT
-A INPUT -m state --state NEW -m udp -p udp --dport 111 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 875 -j ACCEPT
-A INPUT -m state --state NEW -m udp -p udp --dport 875 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 2049 -j ACCEPT
-A INPUT -m state --state NEW -m udp -p udp --dport 2049 -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
COMMIT
</screen>
				</para>
				<para>
				Please remember to restart iptables after any change on that file:
<screen>
service iptables restart
</screen>
				</para>
				</listitem>
			</itemizedlist>  <!-- iptables/firewalld -->
		</listitem>
		<listitem>
			<para>
			Check the security group (see <xref linkend="SecurityGroups"/>): 
			access to ports 111, 875 and 2049 (IPv4 Ingress both TCP and UDP) should be guaranteed: 
			<mediaobject>
			<imageobject>
  				<imagedata condition="web" fileref="./images/AccessAndSecurity.jpg" />
  				<imagedata condition="pdf" fileref="./images/AccessAndSecurity.jpg" width="450" />
			</imageobject>
			</mediaobject>
			</para>
		</listitem>
		<listitem>
			<para>
			Restart nfs server using:
<screen>
systemctl restart nfs
</screen>
			or
<screen>
service nfs restart
</screen>
			</para>
		</listitem>
	</orderedlist>  <!-- configure NFS server -->
</listitem>

<listitem>
	<para><literal>Configure client machine(s)</literal></para>
	<orderedlist>  <!-- configure client -->
		<listitem>
			<para>
			To mount the volume on the other VMs, check that the 
			package providing the <command>mount.nfs</command> command is installed:
			</para>
<screen>
# yum whatprovides "*/mount.nfs"
Loaded plugins: dellsysid, fastestmirror
Loading mirror speeds from cached hostfile
 * base: centos.fastbull.org
 * epel: mirror.23media.de
 * extras: centos.fastbull.org
 * updates: fr2.rpmfind.net
1:nfs-utils-1.3.0-0.8.el7.x86_64 : NFS utilities and supporting clients and daemons for the kernel NFS server
Repo        : base
Matched from:
Filename    : /sbin/mount.nfs



1:nfs-utils-1.3.0-0.8.el7.x86_64 : NFS utilities and supporting clients and daemons for the kernel NFS server
Repo        : @base
Matched from:
Filename    : /sbin/mount.nfs

# yum install nfs-utils
</screen>
		</listitem>
		<listitem>
			<para>
			issue a mount command such as this one (assuming 10.67.1.4 is the NFS server):
<screen>
mount -t nfs 10.67.1.4:/dataNfs /mnt
</screen>
			</para>
		</listitem>
	</orderedlist> <!-- configure client -->
</listitem>
</orderedlist>

<warning><para>                                                                              
It is highly suggested that the VM acting as NFS server is instantiated with                 
 enough resources                                                                            
(at least 2 VCPUs and 4 GB of RAM). Moreover it should be used only for                      
hosting the NFS server (i.e. it shouldn't be used for other activities).                     
</para></warning>


<note><para>
			Please note that this procedure can be used to mount a volume also on hosts
			outside the Cloud: it is sufficient to specify the IP address of these
			hosts in the /etc/exports file on the instance acting as NFS server.
</para></note>






</section>
</section>


<section id="AccessingExternalStorage">
<title>Accessing storage external to the Cloud</title>

<para>
As explained in <xref linkend="NetworkAccess"/> from an instance of the Cloud
by default it is not possible to access a host/service hosted in INFN Padova or 
Legnaro. This also means that by default on a virtual machine of
the CloudVeneto it is not possible to mount a file system 
exported from a storage server hosted in INFN Padova or Legnaro.
</para>


</section>

</chapter>
